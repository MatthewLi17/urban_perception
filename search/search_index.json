{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"City-wide perceptions of neighbourhood quality using street-view images: a methodological toolkit Description We provide all the necessary components to launch your own urban perceptions survey using street view images. This work is part of Emily Muller's PhD funded by the MRC Center for Environment & Health. System Requirements Program Version Python >= 3.7 QGIS >= 3.14.1-Pi GNU parallel Postgresql >= 11.17 Node.js >= 16.13.2 Docker >= 20.10.18 kubectl ==v1.20.2 wandb Python and npm modules are included in requirement.txt files. How to Use this Repository This repository has 3 core components: Title Description Location Key Components Objectives Introduction to Environmental Health and Imagery This is a short video, introducing the domain, methods and describing some pioneering work in this field Video Introduction to the field. Understand different methods. Understand different types of data. Be aware of seminal research. download_images This repository contains all the code needed to download google street view images. download_images Get all available images from Google Street View. Select images to download. web_app This repository contains all the code needed to build a web-app to survey user pairwise image ratins. web_app Build database, back-end and front-end. Dockerise and host using kubernetes. deep_cnn This module contains all the code needed to fine-tune a deep neural network. deep_cnn Use terminal for executing python scripts Train a PyTorch model and visualise results. Implement bash script. Iterate on model hyperparameters to optimise model. Getting started Clone this repository into your local drive. git clone https://github.com/emilymuller1991/urban-perceptions.git cd urban-perceptions Setting up a virtual environment We will set up a virtual environment for running our scripts. In this case, installing specific package versions will not interfere with other programmes we run locally as the environment is contained. Initially, let's set up a virtual environment: virtualenv --python = python3.7 venv This will create a new folder for the virtual environment named venv` in your repository. We activate this environment by running source venv/bin/activate We install dependencies by running pip install requirements.txt Note this will only install python dependencies. We will also need other software utilies at various stages throughout this procotol. Setting up the development virtual environment The pytest and pre-commit module is required for running tests and formatting. This can be installed by running: pip install requirements_dev.txt Now run the tests below to make sure everything is set up correctly. Then, proceed to the video. Testing To run all tests, install pytest . After installing, run pytest tests/ -v","title":"Home"},{"location":"index.html#city-wide-perceptions-of-neighbourhood-quality-using-street-view-images-a-methodological-toolkit","text":"","title":"City-wide perceptions of neighbourhood quality using street-view images: a methodological toolkit"},{"location":"index.html#description","text":"We provide all the necessary components to launch your own urban perceptions survey using street view images. This work is part of Emily Muller's PhD funded by the MRC Center for Environment & Health.","title":"Description"},{"location":"index.html#system-requirements","text":"Program Version Python >= 3.7 QGIS >= 3.14.1-Pi GNU parallel Postgresql >= 11.17 Node.js >= 16.13.2 Docker >= 20.10.18 kubectl ==v1.20.2 wandb Python and npm modules are included in requirement.txt files.","title":"System Requirements"},{"location":"index.html#how-to-use-this-repository","text":"This repository has 3 core components: Title Description Location Key Components Objectives Introduction to Environmental Health and Imagery This is a short video, introducing the domain, methods and describing some pioneering work in this field Video Introduction to the field. Understand different methods. Understand different types of data. Be aware of seminal research. download_images This repository contains all the code needed to download google street view images. download_images Get all available images from Google Street View. Select images to download. web_app This repository contains all the code needed to build a web-app to survey user pairwise image ratins. web_app Build database, back-end and front-end. Dockerise and host using kubernetes. deep_cnn This module contains all the code needed to fine-tune a deep neural network. deep_cnn Use terminal for executing python scripts Train a PyTorch model and visualise results. Implement bash script. Iterate on model hyperparameters to optimise model.","title":"How to Use this Repository"},{"location":"index.html#getting-started","text":"Clone this repository into your local drive. git clone https://github.com/emilymuller1991/urban-perceptions.git cd urban-perceptions","title":"Getting started"},{"location":"index.html#setting-up-a-virtual-environment","text":"We will set up a virtual environment for running our scripts. In this case, installing specific package versions will not interfere with other programmes we run locally as the environment is contained. Initially, let's set up a virtual environment: virtualenv --python = python3.7 venv This will create a new folder for the virtual environment named venv` in your repository. We activate this environment by running source venv/bin/activate We install dependencies by running pip install requirements.txt Note this will only install python dependencies. We will also need other software utilies at various stages throughout this procotol.","title":"Setting up a virtual environment"},{"location":"index.html#setting-up-the-development-virtual-environment","text":"The pytest and pre-commit module is required for running tests and formatting. This can be installed by running: pip install requirements_dev.txt Now run the tests below to make sure everything is set up correctly. Then, proceed to the video.","title":"Setting up the development virtual environment"},{"location":"index.html#testing","text":"To run all tests, install pytest . After installing, run pytest tests/ -v","title":"Testing"},{"location":"1.download_images.html","text":"How To Download Google Street View Images This folder contains all the necessary scripts and steps to take to download Street View Images (GSV) using the Google API. The process can be broken down into the following stages: Obtain Google street view API keys Download city shape file Download roads shape file Create grid points from shape file (QGIS) Get street view metadata and unique panoramic ids (panoid) (Python, GNU) Sample points from roads (python, QGIS) Add azimuths to road vertices (python, QGIS) Merge panoids to azimuths (postgres) Merge road points to panoids + azimuths (postgres) Download images (Python, GNU) The image above shows Greater London Authority area, with road network sampled points (dark green) and downloaded images (light green) for one year. Requirements Program Version Python >= 3.7 QGIS >= 3.14.1-Pi GNU parallel Postgresql >= 11.17 Downloading images Source Files (steps 1-3) Sources are listed below. These files will need to be downloaded manually for London or for a chosen target city. In addition, Street View API keys will need to be generated, see here . Dataset Description Source Generalisability Date downloaded Path London Administrative Boundary Shape file delineating London as an administrative unity https://data.gov.uk/dataset/dda6bd34-7532-48f5-80a7-80b858d57f2b/local-authority-districts-december-2017-full-extent-boundaries-in-united-kingdom-wgs84 Any shape file converted to ESPG:4326 2019 city_admin/greater_london_4326.shp London Roads Shape file of road network https://www.ordnancesurvey.co.uk/business-government/products/open-map-roads Any road network clipped to that same extent as administrative boundary 2019 city_roads/gla_streets.shp API keys api_keys are collected from the Google Cloud Platform and enumerated line by line in a file titled api_keys_original.csv. Another copy is made called api_keys.csv https://console.cloud.google.com/ File needs to be made using Google Cloude Platform user credentials 2019 api_keys/api_keys.csv and api_keys/api_keys_original.csv The folder has the following structure: source \u2502 sources.md \u2514\u2500\u2500\u2500api_keys \u2502 \u2514\u2500\u2500\u2500api_keys.csv \u2502 \u2514\u2500\u2500\u2500api_keys_original.csv \u2514\u2500\u2500\u2500city_admin | \u2514\u2500\u2500\u2500city_4326.shp \u2514\u2500\u2500\u2500city_road \u2514\u2500\u2500\u2500city_roads.shp Get grid points, (lat, lon) pairs, from admin shape file The purpose of this step is to sample the shape file at gridded intervals to create 20m spaced (lat,lon) pairs. These will then serve as input for parallel_gridy.py to obtain street view metadata. Import city shape file in QGIS. Vector > Research Tools > Create Grid (input: admin shape file; parameters: horizontal spacing in degrees; output: grid). Vector > Clip (input: grid; parameters: clip to admin shape file; output: clipped grid to city shape file). Right Click > Export layer > as.csv (output: download_images/outputs/points/city_20m.csv). download_images | ... | \u2514\u2500\u2500\u2500outputs \u2514\u2500\u2500\u2500points \u2502 \u2514\u2500\u2500\u2500city_20m.csv \u2514\u2500\u2500\u2500metadata \u2502 \u2514\u2500\u2500\u2500parallel \u2502 \u2514\u2500\u2500\u2500years \u2514\u2500\u2500\u2500functions ... Get street view metadata In order to make requests to the Google server, we require an API key. When requesting metadata, there is no limit to the number of requests which can be made, although there is a a latency limit. Google provides $200 of credit each month for each API key, which is equivalent to 25K images. Store your API keys in sources/api_keys/ making an original and a copy. The copy will get depleted when running the metadata tasks and must be recopied for each task, subject to cost constraints. The python programme parallel_grid.py will read in API keys in order to obtain metadata. This step will make use of GNU parallel to speed up processing time. Feeding as input parallel chunks of the (lat, lon) pairs to the python programme parallel_gridy.py, which then aggregates image metadata into multiple dictionaries. Our outputs are large dictionaries of panoids of the format {'panoid': [lat, lon, month, year]}. The function includes a filter on images which are not owned by Google and which do not have Status: OK. Part 1 Create a new directory outputs/metadata/parallel. This will store the printed output from the parallel python metadata scrape. Make sure gnu parallel is installed and run: cat outputs/points/city_20m.csv | parallel --delay 1.5 --joblog /tmp/log --progress --pipe --block 3M --files --tmpdir outputs/metadata/parallel python3 functions/parallel_grid.py outputs/city_20m.csv is stdin. parallel calls gnu parallel --delay 1.5 creates a 1.5 second delay in sending each parallel chunk to be processed. This is used to feed in each API key. --joblog prints log to '/tmp/log' and tells the time (important! copy and paste into city folder after execution!) --progress prints job progress to terminal --block 1M splits stdin into 1M size chunks (make sure there are enough API keys for each block or make blocks bigger) --files prints location --tmdir path to save location of output files --python3 parallel_grid.py calls the function to be executed. !NB for dev, remove --files -tmpdir... and replace with --ungroup and retrieve output to terminal. Part 2 Create a new directory in outputs/metadata/years. This will store the aggregated output of the parallel script separated by years. run: python3 functions/parallel_output.py city_20m outputs/metadata/parallel/ /outputs/metadata/ The first argument is the city_20m name given tofiles. The second argument is the path to parallel metadata output and the third argument is path to save aggregated metadata. Sample points from roads get_road_points.py will sample 20m distance points along all roads in the city_street.shp file and output a .shp file containing sampled points. The function uses the pyQGIS module and requires qchainage plugin from QGIS. python3 functions/get_road_points.py source/city_roads/city_streets.shp outputs/roads/city_road_points_20m.shp The first argument inputs road shape file location and the second argument passes save file and location. There are no tests for this function since the qgis plugin is not a python module itself therefore must be imported from QGIS plugins path (amend as necessary in file). Python version must mast python version for QGIS. Open this shape file in QGIS and export as .csv for step 8. Add azimuth to road vertices One step in data cleaning is to get the azimuth angle of the road to north bearing. This will then serve as a rotation of camera input when we download the images. This will be necessary since inputting standard 90 degrees is often offset and does not give a perpindicular angle to the road. The azimuth.py function uses the pyQGIS module and requires Azimuth plugin from QGIS. Import city_roads shape file into QGIS Vector > Geometry > Extract Vertices (input: city_road.shp; output: Takes a vector layer and generates a point layer with points representing the vertices in the input geometries) Vector > Geometry > Add Geometry Attributes (input: output from previous step; output: Computes geometric properties of the features in a vector layer and includes them in the output layer) Export layer > as .shp (output: outputs/roads/city_road_vertices_geom.csv) In the final step we will call the function azimuth.py to get azimuth angles from vertices in shape file and merge back to our exported layer. python3 azimuth.py source/city_roads/city_streets.shp** outputs/roads/city_road_vertices_geom.csv outputs/roads/ The first argument is the location of the streets shape file, the second argument is the location of our QGIS output and third argument is save location of our new azimuth file. **city_streets.shp file needs to be single parts. Merge panoids to azimuth The final steps utilise psql to merge panoid metadata to azimuths to serve as input for download. It requires the installation of postgres. Once you have downloaded postgres and created user, create database as follows: sudo su - postgres psql CREATE DATABASE city; psql -U user -d city -f functions/panoids_azimuth_merge.sql Change path to root_dir in lines 35 and 39. Merge road points to panoids + azimuths The next step is to sample panoids by the 20m road intervals. This ensures one area is not over sampled, and furthermore, have an approximation of GSV road coverage. psql -U user -d city -f functions/roads_panoids_merge.sql remove duplicate IDs by specifying path in function/remove_duplicate_ids and run python3 functions/remove_duplicate_ids.py Download images Now we can download the images using the final output from the previous step, python and gnu parallel. Remember to copy back the original api_keys_original.csv to api_keys.csv. We will first convert the sampled panoids to plain text as follows: split -l 40000000 -d city_road_sample_panoids_unique.csv to_download/city from to_download/city remove the header manually. We are now ready to download the images. In get_images.py append the save path to suit your own local or remote drive. Ensure that there is enough space on your drive (each image is around 600kB) and run cat outputs/psql/to_download/city | parallel --delay 1.5 --joblog /tmp/log --pipe --block 2M --ungroup python3 functions/get_images.py This function will download 2 angles per panoid, facing either side of the street. When blocking files for input, 2MB should ensure that each API key will download 25K images.","title":"How To Download Google Street View Images"},{"location":"1.download_images.html#how-to-download-google-street-view-images","text":"This folder contains all the necessary scripts and steps to take to download Street View Images (GSV) using the Google API. The process can be broken down into the following stages: Obtain Google street view API keys Download city shape file Download roads shape file Create grid points from shape file (QGIS) Get street view metadata and unique panoramic ids (panoid) (Python, GNU) Sample points from roads (python, QGIS) Add azimuths to road vertices (python, QGIS) Merge panoids to azimuths (postgres) Merge road points to panoids + azimuths (postgres) Download images (Python, GNU) The image above shows Greater London Authority area, with road network sampled points (dark green) and downloaded images (light green) for one year.","title":"How To Download Google Street View Images"},{"location":"1.download_images.html#requirements","text":"Program Version Python >= 3.7 QGIS >= 3.14.1-Pi GNU parallel Postgresql >= 11.17","title":"Requirements"},{"location":"1.download_images.html#downloading-images","text":"","title":"Downloading images"},{"location":"1.download_images.html#source-files-steps-1-3","text":"Sources are listed below. These files will need to be downloaded manually for London or for a chosen target city. In addition, Street View API keys will need to be generated, see here . Dataset Description Source Generalisability Date downloaded Path London Administrative Boundary Shape file delineating London as an administrative unity https://data.gov.uk/dataset/dda6bd34-7532-48f5-80a7-80b858d57f2b/local-authority-districts-december-2017-full-extent-boundaries-in-united-kingdom-wgs84 Any shape file converted to ESPG:4326 2019 city_admin/greater_london_4326.shp London Roads Shape file of road network https://www.ordnancesurvey.co.uk/business-government/products/open-map-roads Any road network clipped to that same extent as administrative boundary 2019 city_roads/gla_streets.shp API keys api_keys are collected from the Google Cloud Platform and enumerated line by line in a file titled api_keys_original.csv. Another copy is made called api_keys.csv https://console.cloud.google.com/ File needs to be made using Google Cloude Platform user credentials 2019 api_keys/api_keys.csv and api_keys/api_keys_original.csv The folder has the following structure: source \u2502 sources.md \u2514\u2500\u2500\u2500api_keys \u2502 \u2514\u2500\u2500\u2500api_keys.csv \u2502 \u2514\u2500\u2500\u2500api_keys_original.csv \u2514\u2500\u2500\u2500city_admin | \u2514\u2500\u2500\u2500city_4326.shp \u2514\u2500\u2500\u2500city_road \u2514\u2500\u2500\u2500city_roads.shp","title":"Source Files (steps 1-3)"},{"location":"1.download_images.html#get-grid-points-lat-lon-pairs-from-admin-shape-file","text":"The purpose of this step is to sample the shape file at gridded intervals to create 20m spaced (lat,lon) pairs. These will then serve as input for parallel_gridy.py to obtain street view metadata. Import city shape file in QGIS. Vector > Research Tools > Create Grid (input: admin shape file; parameters: horizontal spacing in degrees; output: grid). Vector > Clip (input: grid; parameters: clip to admin shape file; output: clipped grid to city shape file). Right Click > Export layer > as.csv (output: download_images/outputs/points/city_20m.csv). download_images | ... | \u2514\u2500\u2500\u2500outputs \u2514\u2500\u2500\u2500points \u2502 \u2514\u2500\u2500\u2500city_20m.csv \u2514\u2500\u2500\u2500metadata \u2502 \u2514\u2500\u2500\u2500parallel \u2502 \u2514\u2500\u2500\u2500years \u2514\u2500\u2500\u2500functions ...","title":"Get grid points, (lat, lon) pairs, from admin shape file"},{"location":"1.download_images.html#get-street-view-metadata","text":"In order to make requests to the Google server, we require an API key. When requesting metadata, there is no limit to the number of requests which can be made, although there is a a latency limit. Google provides $200 of credit each month for each API key, which is equivalent to 25K images. Store your API keys in sources/api_keys/ making an original and a copy. The copy will get depleted when running the metadata tasks and must be recopied for each task, subject to cost constraints. The python programme parallel_grid.py will read in API keys in order to obtain metadata. This step will make use of GNU parallel to speed up processing time. Feeding as input parallel chunks of the (lat, lon) pairs to the python programme parallel_gridy.py, which then aggregates image metadata into multiple dictionaries. Our outputs are large dictionaries of panoids of the format {'panoid': [lat, lon, month, year]}. The function includes a filter on images which are not owned by Google and which do not have Status: OK.","title":"Get street view metadata"},{"location":"1.download_images.html#part-1","text":"Create a new directory outputs/metadata/parallel. This will store the printed output from the parallel python metadata scrape. Make sure gnu parallel is installed and run: cat outputs/points/city_20m.csv | parallel --delay 1.5 --joblog /tmp/log --progress --pipe --block 3M --files --tmpdir outputs/metadata/parallel python3 functions/parallel_grid.py outputs/city_20m.csv is stdin. parallel calls gnu parallel --delay 1.5 creates a 1.5 second delay in sending each parallel chunk to be processed. This is used to feed in each API key. --joblog prints log to '/tmp/log' and tells the time (important! copy and paste into city folder after execution!) --progress prints job progress to terminal --block 1M splits stdin into 1M size chunks (make sure there are enough API keys for each block or make blocks bigger) --files prints location --tmdir path to save location of output files --python3 parallel_grid.py calls the function to be executed. !NB for dev, remove --files -tmpdir... and replace with --ungroup and retrieve output to terminal.","title":"Part 1"},{"location":"1.download_images.html#part-2","text":"Create a new directory in outputs/metadata/years. This will store the aggregated output of the parallel script separated by years. run: python3 functions/parallel_output.py city_20m outputs/metadata/parallel/ /outputs/metadata/ The first argument is the city_20m name given tofiles. The second argument is the path to parallel metadata output and the third argument is path to save aggregated metadata.","title":"Part 2"},{"location":"1.download_images.html#sample-points-from-roads","text":"get_road_points.py will sample 20m distance points along all roads in the city_street.shp file and output a .shp file containing sampled points. The function uses the pyQGIS module and requires qchainage plugin from QGIS. python3 functions/get_road_points.py source/city_roads/city_streets.shp outputs/roads/city_road_points_20m.shp The first argument inputs road shape file location and the second argument passes save file and location. There are no tests for this function since the qgis plugin is not a python module itself therefore must be imported from QGIS plugins path (amend as necessary in file). Python version must mast python version for QGIS. Open this shape file in QGIS and export as .csv for step 8.","title":"Sample points from roads"},{"location":"1.download_images.html#add-azimuth-to-road-vertices","text":"One step in data cleaning is to get the azimuth angle of the road to north bearing. This will then serve as a rotation of camera input when we download the images. This will be necessary since inputting standard 90 degrees is often offset and does not give a perpindicular angle to the road. The azimuth.py function uses the pyQGIS module and requires Azimuth plugin from QGIS. Import city_roads shape file into QGIS Vector > Geometry > Extract Vertices (input: city_road.shp; output: Takes a vector layer and generates a point layer with points representing the vertices in the input geometries) Vector > Geometry > Add Geometry Attributes (input: output from previous step; output: Computes geometric properties of the features in a vector layer and includes them in the output layer) Export layer > as .shp (output: outputs/roads/city_road_vertices_geom.csv) In the final step we will call the function azimuth.py to get azimuth angles from vertices in shape file and merge back to our exported layer. python3 azimuth.py source/city_roads/city_streets.shp** outputs/roads/city_road_vertices_geom.csv outputs/roads/ The first argument is the location of the streets shape file, the second argument is the location of our QGIS output and third argument is save location of our new azimuth file. **city_streets.shp file needs to be single parts.","title":"Add azimuth to road vertices"},{"location":"1.download_images.html#merge-panoids-to-azimuth","text":"The final steps utilise psql to merge panoid metadata to azimuths to serve as input for download. It requires the installation of postgres. Once you have downloaded postgres and created user, create database as follows: sudo su - postgres psql CREATE DATABASE city; psql -U user -d city -f functions/panoids_azimuth_merge.sql Change path to root_dir in lines 35 and 39.","title":"Merge panoids to azimuth"},{"location":"1.download_images.html#merge-road-points-to-panoids-azimuths","text":"The next step is to sample panoids by the 20m road intervals. This ensures one area is not over sampled, and furthermore, have an approximation of GSV road coverage. psql -U user -d city -f functions/roads_panoids_merge.sql remove duplicate IDs by specifying path in function/remove_duplicate_ids and run python3 functions/remove_duplicate_ids.py","title":"Merge road points to panoids + azimuths"},{"location":"1.download_images.html#download-images","text":"Now we can download the images using the final output from the previous step, python and gnu parallel. Remember to copy back the original api_keys_original.csv to api_keys.csv. We will first convert the sampled panoids to plain text as follows: split -l 40000000 -d city_road_sample_panoids_unique.csv to_download/city from to_download/city remove the header manually. We are now ready to download the images. In get_images.py append the save path to suit your own local or remote drive. Ensure that there is enough space on your drive (each image is around 600kB) and run cat outputs/psql/to_download/city | parallel --delay 1.5 --joblog /tmp/log --pipe --block 2M --ungroup python3 functions/get_images.py This function will download 2 angles per panoid, facing either side of the street. When blocking files for input, 2MB should ensure that each API key will download 25K images.","title":"Download images"},{"location":"2.web_app.html","text":"Creating and Deploying a Web-App In this section, we will go through the necessary steps for creating a web-application using React front-end and Flask back-end. Our database of images and perception rating will be hosted in postgres. After which, we will deploy the entire app using Digital Ocean Droplet and Kubernetes services. Web-App with Postgres Database using Flask + React In this example, we will create a web app which displays two images side by side and the user is asked to select one, adding their rating to the database. For more detailed instructions follow this blog . Summary I Postgres Database Flask back-end React front-end Requirements I Program Version Python >= 3.7 Postgresql >= 11.17 Node.js and npm >= 16.13.2 API key from Google Console . Postgres DB Create DB example as user postgres : psql postgres postgres=# CREATE DATABASE example; From web_app/sql/ dump ready-made data, ratings and images , into the postgres database example : web-app/sql$ psql -U postgres example < ratings web-app/sql$ psql -U postgres example < images Flask Back-end Set environmental variables: export app_host='localhost' export db='example' export db_user='postgres' export db_host='localhost' export db_port=5432 run: web-app/back_end$ python app.py React Front-end Set environmental variables in the new file web-app/front_end/.env: REACT_APP_BACK_END_HOST ='localhost' REACT_APP_BACK_END_PORT = '5000' REACT_APP_API_KEY = 'API_KEY_from_GOOGLE_STREET_VIEW_API' Using an api key from Google Console . run: web-app/front_end$ npm install run: web-app/front_end$ npm start Deployment using DigitalOcean Droplet and Kubernetes Cluster In this section we will go through the steps needed to deploy your application using Digital Ocean cloud infrastructure. Summary II Moving database to Droplet Dockerise front-end and back-end Deployment using Kubernetes Requirements II Program Version Python >= 3.7 Postgresql >= 11.17 Node.js >= 16.13.2 Docker >= 20.10.18 kubectl ==v1.20.2 A web domain . DockerHub credentials DigitalOcean, Droplet + Kubernetes Cluster Moving database to Droplet Once you have created a Droplet, configured your root password and created a new user, your droplet IP address will show. You can ssh into your droplet from your local machine: ssh user@IP.address In your Droplet, install postgres and postgres-client (below). Create example database: sudo apt-get install postgresql-client psql postgres postgres=# CREATE DATABASE example; \\q We'll now copy the local file, into_droplet , to the remote Droplet using sftp . urban_perceptions/web_app/sql$ sftp user@IP.address sftp> put into_droplet sftp? Ctrl + D If you ls in the directory, you will see the file into_droplet has copied over to your Digital Ocean Droplet. ssh back into your droplet and pg_dump the into_droplet file into your example database: ssh user@IP.address droplet$ psql example < into_droplet droplet$ psql -d example example=# \\dn+ You should see a list of schemas in the database. Dockerise front-end and back-end In this section we will dockerise our front-ends and back-ends. npm run-script build docker build --no-cache --pull --force-rm -t front-end-example . Inspect docker images using 'docker images', you should now see front-end-example as the latest image. Run the image using: docker run -it -p 4000:80 front-end-example:latest This should launch the React app at the localhost:4000 . We'll now push the docker image to DockerHub. Get the image tag from 'docker images' and tag it: docker tag [IMAGEID] username/example-front-end docker push username/example-front-end If you visit DockerHub, you should now see the image available on the server. Let's follow similar steps for the back-end. Within the back-end dir enter: docker build --no-cache --pull --force-rm -t back-end-example . Once this is complete run the image: docker run -it --network=\"host\" --env db_host='localhost' --env db_port=5432 --env db_root_password='[POSTGRES_PASSWORD]' back-end:latest /bin/bash This will require you to update your local postgres settings so listen to on all addresses . Continue to push the docker image to DockerHub as with front-end. Deployment with Kubernetes Deploy database Once you have your DigitalOcean Kubernetes cluster and kubectl command line tool, we will begin by connecting the cluster to our external postgres db. To do so run: sudo kubectl apply -f sql/db.yaml where PRIVATE_IP_DROPLET in the db.yaml file is configured to your digital ocean droplet private IP address. Once this is configured, run: sudo kubectl describe services --namespace postgres to get the IP address of the postgres endpoint. We will need this to configure postgres on the droplet to listen to and to connect to the postgres DB from the back-end. Configuring postgres settings We will have to configure remote access to postgres in the Droplet by changing some configuration files . First change the listening address to listen_addresses = '*': vim /etc/postgresql/13/main/postgresql.conf Then, add the postgres endpoint IP to the postgres config file: vim /etc/postgresql/13/main/pg_hba.conf adding under IPV4 addresses the following: host all all [POSTGRES_ENDPOINT_IP]/24 md5 If there are any issues with the password, then these can be amended by changing md5 to trust. Deploying back-end Now that we have configured out postgres endpoint from the cluster to the droplet, we will deploy the backend. In back_end/back-end.yml configure username/BACK_END_CONTAINER, POSTGRES_ENDPOINT_IP, POSTGRES_USER_PASSWORD. Once that is complete run: sudo kubectl apply -f back-end.yml This will create a back-end deployment, and a back-end service. The back-end service IP will be needed for the front-end to request from. Make a note of the external IP by running: sudo kubectl get svc Route DNS Configure your DNS registrar to point to Digital Ocean from the site where you purchased your web domain. Deploy front-end In front_end/front-end-ingress.yml configure WEB_DOMAIN and username/FRONT_END_CONTAINER. Then run: sudo kubectl apply -f front-end-ingress.yml Expose the front-end deployment to a LoadBalancer (which we will use in the next step to route IP): sudo kubectl expose deployment front-end --type=LoadBalancer --name=[name-of-load-balancer] Get the external IP address of the LoadBalancer using sudo kubectl get svc Create a new A type DNS record in Digital Ocean that is linked to the LoadBalancer's external IP address. Finally, create a configmap which will host environmental variables for the React App. kubectl create configmap front-end-config --from-file=config.js=dev.properties --dry-run -o yaml \\ | kubectl apply -f -","title":"Creating and Deploying a Web-App"},{"location":"2.web_app.html#creating-and-deploying-a-web-app","text":"In this section, we will go through the necessary steps for creating a web-application using React front-end and Flask back-end. Our database of images and perception rating will be hosted in postgres. After which, we will deploy the entire app using Digital Ocean Droplet and Kubernetes services.","title":"Creating and Deploying a Web-App"},{"location":"2.web_app.html#web-app-with-postgres-database-using-flask-react","text":"In this example, we will create a web app which displays two images side by side and the user is asked to select one, adding their rating to the database. For more detailed instructions follow this blog .","title":"Web-App with Postgres Database using Flask +\u00a0React"},{"location":"2.web_app.html#summary-i","text":"Postgres Database Flask back-end React front-end","title":"Summary I"},{"location":"2.web_app.html#requirements-i","text":"Program Version Python >= 3.7 Postgresql >= 11.17 Node.js and npm >= 16.13.2 API key from Google Console .","title":"Requirements I"},{"location":"2.web_app.html#postgres-db","text":"Create DB example as user postgres : psql postgres postgres=# CREATE DATABASE example; From web_app/sql/ dump ready-made data, ratings and images , into the postgres database example : web-app/sql$ psql -U postgres example < ratings web-app/sql$ psql -U postgres example < images","title":"Postgres DB"},{"location":"2.web_app.html#flask-back-end","text":"Set environmental variables: export app_host='localhost' export db='example' export db_user='postgres' export db_host='localhost' export db_port=5432 run: web-app/back_end$ python app.py","title":"Flask Back-end"},{"location":"2.web_app.html#react-front-end","text":"Set environmental variables in the new file web-app/front_end/.env: REACT_APP_BACK_END_HOST ='localhost' REACT_APP_BACK_END_PORT = '5000' REACT_APP_API_KEY = 'API_KEY_from_GOOGLE_STREET_VIEW_API' Using an api key from Google Console . run: web-app/front_end$ npm install run: web-app/front_end$ npm start","title":"React Front-end"},{"location":"2.web_app.html#deployment-using-digitalocean-droplet-and-kubernetes-cluster","text":"In this section we will go through the steps needed to deploy your application using Digital Ocean cloud infrastructure.","title":"Deployment using DigitalOcean Droplet and Kubernetes Cluster"},{"location":"2.web_app.html#summary-ii","text":"Moving database to Droplet Dockerise front-end and back-end Deployment using Kubernetes","title":"Summary II"},{"location":"2.web_app.html#requirements-ii","text":"Program Version Python >= 3.7 Postgresql >= 11.17 Node.js >= 16.13.2 Docker >= 20.10.18 kubectl ==v1.20.2 A web domain . DockerHub credentials DigitalOcean, Droplet + Kubernetes Cluster","title":"Requirements II"},{"location":"2.web_app.html#moving-database-to-droplet","text":"Once you have created a Droplet, configured your root password and created a new user, your droplet IP address will show. You can ssh into your droplet from your local machine: ssh user@IP.address In your Droplet, install postgres and postgres-client (below). Create example database: sudo apt-get install postgresql-client psql postgres postgres=# CREATE DATABASE example; \\q We'll now copy the local file, into_droplet , to the remote Droplet using sftp . urban_perceptions/web_app/sql$ sftp user@IP.address sftp> put into_droplet sftp? Ctrl + D If you ls in the directory, you will see the file into_droplet has copied over to your Digital Ocean Droplet. ssh back into your droplet and pg_dump the into_droplet file into your example database: ssh user@IP.address droplet$ psql example < into_droplet droplet$ psql -d example example=# \\dn+ You should see a list of schemas in the database.","title":"Moving database to Droplet"},{"location":"2.web_app.html#dockerise-front-end-and-back-end","text":"In this section we will dockerise our front-ends and back-ends. npm run-script build docker build --no-cache --pull --force-rm -t front-end-example . Inspect docker images using 'docker images', you should now see front-end-example as the latest image. Run the image using: docker run -it -p 4000:80 front-end-example:latest This should launch the React app at the localhost:4000 . We'll now push the docker image to DockerHub. Get the image tag from 'docker images' and tag it: docker tag [IMAGEID] username/example-front-end docker push username/example-front-end If you visit DockerHub, you should now see the image available on the server. Let's follow similar steps for the back-end. Within the back-end dir enter: docker build --no-cache --pull --force-rm -t back-end-example . Once this is complete run the image: docker run -it --network=\"host\" --env db_host='localhost' --env db_port=5432 --env db_root_password='[POSTGRES_PASSWORD]' back-end:latest /bin/bash This will require you to update your local postgres settings so listen to on all addresses . Continue to push the docker image to DockerHub as with front-end.","title":"Dockerise front-end and back-end"},{"location":"2.web_app.html#deployment-with-kubernetes","text":"","title":"Deployment with Kubernetes"},{"location":"2.web_app.html#deploy-database","text":"Once you have your DigitalOcean Kubernetes cluster and kubectl command line tool, we will begin by connecting the cluster to our external postgres db. To do so run: sudo kubectl apply -f sql/db.yaml where PRIVATE_IP_DROPLET in the db.yaml file is configured to your digital ocean droplet private IP address. Once this is configured, run: sudo kubectl describe services --namespace postgres to get the IP address of the postgres endpoint. We will need this to configure postgres on the droplet to listen to and to connect to the postgres DB from the back-end.","title":"Deploy database"},{"location":"2.web_app.html#configuring-postgres-settings","text":"We will have to configure remote access to postgres in the Droplet by changing some configuration files . First change the listening address to listen_addresses = '*': vim /etc/postgresql/13/main/postgresql.conf Then, add the postgres endpoint IP to the postgres config file: vim /etc/postgresql/13/main/pg_hba.conf adding under IPV4 addresses the following: host all all [POSTGRES_ENDPOINT_IP]/24 md5 If there are any issues with the password, then these can be amended by changing md5 to trust.","title":"Configuring postgres settings"},{"location":"2.web_app.html#deploying-back-end","text":"Now that we have configured out postgres endpoint from the cluster to the droplet, we will deploy the backend. In back_end/back-end.yml configure username/BACK_END_CONTAINER, POSTGRES_ENDPOINT_IP, POSTGRES_USER_PASSWORD. Once that is complete run: sudo kubectl apply -f back-end.yml This will create a back-end deployment, and a back-end service. The back-end service IP will be needed for the front-end to request from. Make a note of the external IP by running: sudo kubectl get svc","title":"Deploying back-end"},{"location":"2.web_app.html#route-dns","text":"Configure your DNS registrar to point to Digital Ocean from the site where you purchased your web domain.","title":"Route DNS"},{"location":"2.web_app.html#deploy-front-end","text":"In front_end/front-end-ingress.yml configure WEB_DOMAIN and username/FRONT_END_CONTAINER. Then run: sudo kubectl apply -f front-end-ingress.yml Expose the front-end deployment to a LoadBalancer (which we will use in the next step to route IP): sudo kubectl expose deployment front-end --type=LoadBalancer --name=[name-of-load-balancer] Get the external IP address of the LoadBalancer using sudo kubectl get svc Create a new A type DNS record in Digital Ocean that is linked to the LoadBalancer's external IP address. Finally, create a configmap which will host environmental variables for the React App. kubectl create configmap front-end-config --from-file=config.js=dev.properties --dry-run -o yaml \\ | kubectl apply -f -","title":"Deploy front-end"},{"location":"3.deep_cnn.html","text":"Training a Convolutional Neural Network in PyTorch In this section we will fine-tune a pretrained CNN to learn to predict perception scores. Before we dive into the components of the model, we will just get the code up and running with the default settings. Training Locally If you have not yet done so, now is a good time to clone this repository into your local drive (see Getting Started). Let's check the model training runs locally (albeit slowly without a GPU). From urban-perceptions run: python3 -m deep_cnn \\ --epochs = 1 \\ --data_dir = tests/tests3/test_input \\ --wandb = False \\ A brief description of all arguments is returned with the following: python3 -m deep_cnn -h usage: __main__.py [ -h ] [ --epochs EPOCHS ] [ --batch_size BATCH_SIZE ] [ --model MODEL ] [ --pre PRE ] [ --study_id STUDY_ID ] [ --oversample OVERSAMPLE ] [ --root_dir ROOT_DIR ] [ --lr LR ] [ --run_name RUN_NAME ] [ --data_dir DATA_DIR ] [ --wandb WANDB ] optional arguments: -h, --help show this help message and exit --epochs EPOCHS number of training epochs --batch_size BATCH_SIZE batch size for SGD --model MODEL pre-trained model --pre PRE pre-processing for image input --study_id STUDY_ID perceptions from place pulse study --oversample OVERSAMPLE whether to oversample --root_dir ROOT_DIR path to recode-perceptions --lr LR learning rate --run_name RUN_NAME unique name to identify hyperparameter choices --data_dir DATA_DIR path to input data --wandb WANDB track progress in wandb.ai where root_dir is your local path to urban-perceptions, and data_dir is your path to input/pp_images . If you have not yet downloaded the full image dataset, but you want to run this script locally, you can run the test images, tests/test_input/test_images/. You should see the following output in your terminal: Running on cuda device Epoch 0: 2%|\u2588\u2588\u258e | 35/1697 [00:19<15:28, 1.79batch/s, loss=3.06] If you have a GPU locally, you will also see 'Running on cuda device'. However, this will be replaced by CPU if no GPU device is found. The model is training through its first epoch batch by batch. This one epoch is expected to take 15 minutes to complete. Once finished, the full log can be found in the folder outputs/logger/. Hyperparameter Optimisation Let's take a look at the design choices which can be made for model training. Parameter Description Trade Offs References Epochs Determine the number of times to run through the entire training batch. Typically there is an inflection point where decreases in loss are marginal. Continued increase after this reflects overfitting Bias-Variance Trade-Off Batch size Number of samples for each training step Smaller batch sizes increase performance of stochastic gradient descent algorithms while also preserving memory by loading in batches. Larger batch size can speed up computation. Batch-size Learning Rate Step size of parameter update at each iteration A larger learning rate requires fewer epochs but can easily result in unstable results such as local minima. Smaller learning rates can fail to find an optima at all. Adaptive learning rates consider large step sizes in the earlier epochs of training and reduces the step size in later epochs Learning Rates Model Pre-trained model Can vary by size, depth, structure and trainable parameters Smaller models are faster to train while deeper model typically achieve higher levels of abstraction. Models with dropout can avoid overfitting, PyTorch pre-trained models Pre-processing Image pre-processing required for model input Pre-trained models have parameters trained within a given range and perform better when the target dataset distribution is closer matched to the source dataset distribution - There is never a one-rule-fits-all-approach to training a deep neural network. Design choices can be guided by the domain task, the dataset size and distribution, hardware constraints, time constraints or all of the above. The wonder of moden software and computing is that the cycle of iterating on model choices has been sped up enormously: There exists many types of search algorithms for finding the optimal hyperparameters, such as gridded search, random search and Bayesian optimisation. We would like to add a priori the wisdom of the crowd. There has been a lot written about deep learning hyperparameters, so we don't need to go in blind. To help you configure some intervals, consider the following questions How many epochs will ensure you have reached the global minima? How much memory constraints do you have on model and batch size? What ratio of batch size to data samples/classes is considered a benchmark? What is a typical learning rate for similar image classification tasks? How can an adaptive learning rate be implemented? Which model has shown the best performance on benchmark datasets? Is there a PyTorch version to download pre-trained weights? Does this model require certain pre-processing? If yes, you will have to add the preprocessing to dataset_generator.py Once you have tried answered the questions above, you will have a constraint on what is reasonable to test. When performing hyperparameter optimisation, we will not evaluate our test performance during training. This test set will be a hold-out set used only to evaluate the performance of our final model after training. Instead, we evaluate the performance of the model during training on the validation set. This is done typically to avoid overfitting to the test set during hyperparameter optimisation. Our goal in training is to test the generalisability of our training paradigm and the test-set allows us to do this. Hyperparameter optimisation using wandb.ai We will use weights and biases to track our model training and validation. This platform uses a lightweight python package to log metrics during training. To use this, you will need to create an account at https://wandb.ai/site . You can create an account using your GitHub or Gmail. You will be prompted to create a username, keep a note of this. Once you have completed your registration, you will be directed to a landing page with an API key, keep a note of this too. Once you have configured your user credentials, create a new project called recode-perceptions. This folder will log all of our training runs. In order for python to get access to your personal user credentials, you will have to set them as environmental variables which python then accesses using os.getenv(\"WB_KEY\") . Set your environmental variables as follows: export WB_KEY = API_KEY export WB_PROJECT = \"urban-perceptions\" export WB_USER = \"username\" If you now run the scripts with --wandb=True , you should begin to see the metrics being tracked on the platform: Dataset The Place Pulse dataset can be downloaded by executing the data_download.sh script. You will first need to enter your remote path to the urban-perceptions directory (line 5). The datasets were not released by us, and we do not claim any rights on them. Use the datasets at your responsibility and make sure you fulfil the licenses that they were released with. If you use any of the datasets please consider citing the original authors of Place Pulse MIT . Revisit Hyperparameter Optimisation and iterate on model choices using run_name to track design choices.","title":"Training a Convolutional Neural Network in PyTorch"},{"location":"3.deep_cnn.html#training-a-convolutional-neural-network-in-pytorch","text":"In this section we will fine-tune a pretrained CNN to learn to predict perception scores. Before we dive into the components of the model, we will just get the code up and running with the default settings.","title":"Training a Convolutional Neural Network in PyTorch"},{"location":"3.deep_cnn.html#training-locally","text":"If you have not yet done so, now is a good time to clone this repository into your local drive (see Getting Started). Let's check the model training runs locally (albeit slowly without a GPU). From urban-perceptions run: python3 -m deep_cnn \\ --epochs = 1 \\ --data_dir = tests/tests3/test_input \\ --wandb = False \\ A brief description of all arguments is returned with the following: python3 -m deep_cnn -h usage: __main__.py [ -h ] [ --epochs EPOCHS ] [ --batch_size BATCH_SIZE ] [ --model MODEL ] [ --pre PRE ] [ --study_id STUDY_ID ] [ --oversample OVERSAMPLE ] [ --root_dir ROOT_DIR ] [ --lr LR ] [ --run_name RUN_NAME ] [ --data_dir DATA_DIR ] [ --wandb WANDB ] optional arguments: -h, --help show this help message and exit --epochs EPOCHS number of training epochs --batch_size BATCH_SIZE batch size for SGD --model MODEL pre-trained model --pre PRE pre-processing for image input --study_id STUDY_ID perceptions from place pulse study --oversample OVERSAMPLE whether to oversample --root_dir ROOT_DIR path to recode-perceptions --lr LR learning rate --run_name RUN_NAME unique name to identify hyperparameter choices --data_dir DATA_DIR path to input data --wandb WANDB track progress in wandb.ai where root_dir is your local path to urban-perceptions, and data_dir is your path to input/pp_images . If you have not yet downloaded the full image dataset, but you want to run this script locally, you can run the test images, tests/test_input/test_images/. You should see the following output in your terminal: Running on cuda device Epoch 0: 2%|\u2588\u2588\u258e | 35/1697 [00:19<15:28, 1.79batch/s, loss=3.06] If you have a GPU locally, you will also see 'Running on cuda device'. However, this will be replaced by CPU if no GPU device is found. The model is training through its first epoch batch by batch. This one epoch is expected to take 15 minutes to complete. Once finished, the full log can be found in the folder outputs/logger/.","title":"Training Locally"},{"location":"3.deep_cnn.html#hyperparameter-optimisation","text":"Let's take a look at the design choices which can be made for model training. Parameter Description Trade Offs References Epochs Determine the number of times to run through the entire training batch. Typically there is an inflection point where decreases in loss are marginal. Continued increase after this reflects overfitting Bias-Variance Trade-Off Batch size Number of samples for each training step Smaller batch sizes increase performance of stochastic gradient descent algorithms while also preserving memory by loading in batches. Larger batch size can speed up computation. Batch-size Learning Rate Step size of parameter update at each iteration A larger learning rate requires fewer epochs but can easily result in unstable results such as local minima. Smaller learning rates can fail to find an optima at all. Adaptive learning rates consider large step sizes in the earlier epochs of training and reduces the step size in later epochs Learning Rates Model Pre-trained model Can vary by size, depth, structure and trainable parameters Smaller models are faster to train while deeper model typically achieve higher levels of abstraction. Models with dropout can avoid overfitting, PyTorch pre-trained models Pre-processing Image pre-processing required for model input Pre-trained models have parameters trained within a given range and perform better when the target dataset distribution is closer matched to the source dataset distribution - There is never a one-rule-fits-all-approach to training a deep neural network. Design choices can be guided by the domain task, the dataset size and distribution, hardware constraints, time constraints or all of the above. The wonder of moden software and computing is that the cycle of iterating on model choices has been sped up enormously: There exists many types of search algorithms for finding the optimal hyperparameters, such as gridded search, random search and Bayesian optimisation. We would like to add a priori the wisdom of the crowd. There has been a lot written about deep learning hyperparameters, so we don't need to go in blind. To help you configure some intervals, consider the following questions How many epochs will ensure you have reached the global minima? How much memory constraints do you have on model and batch size? What ratio of batch size to data samples/classes is considered a benchmark? What is a typical learning rate for similar image classification tasks? How can an adaptive learning rate be implemented? Which model has shown the best performance on benchmark datasets? Is there a PyTorch version to download pre-trained weights? Does this model require certain pre-processing? If yes, you will have to add the preprocessing to dataset_generator.py Once you have tried answered the questions above, you will have a constraint on what is reasonable to test. When performing hyperparameter optimisation, we will not evaluate our test performance during training. This test set will be a hold-out set used only to evaluate the performance of our final model after training. Instead, we evaluate the performance of the model during training on the validation set. This is done typically to avoid overfitting to the test set during hyperparameter optimisation. Our goal in training is to test the generalisability of our training paradigm and the test-set allows us to do this.","title":"Hyperparameter Optimisation"},{"location":"3.deep_cnn.html#hyperparameter-optimisation-using-wandbai","text":"We will use weights and biases to track our model training and validation. This platform uses a lightweight python package to log metrics during training. To use this, you will need to create an account at https://wandb.ai/site . You can create an account using your GitHub or Gmail. You will be prompted to create a username, keep a note of this. Once you have completed your registration, you will be directed to a landing page with an API key, keep a note of this too. Once you have configured your user credentials, create a new project called recode-perceptions. This folder will log all of our training runs. In order for python to get access to your personal user credentials, you will have to set them as environmental variables which python then accesses using os.getenv(\"WB_KEY\") . Set your environmental variables as follows: export WB_KEY = API_KEY export WB_PROJECT = \"urban-perceptions\" export WB_USER = \"username\" If you now run the scripts with --wandb=True , you should begin to see the metrics being tracked on the platform:","title":"Hyperparameter optimisation using wandb.ai"},{"location":"3.deep_cnn.html#dataset","text":"The Place Pulse dataset can be downloaded by executing the data_download.sh script. You will first need to enter your remote path to the urban-perceptions directory (line 5). The datasets were not released by us, and we do not claim any rights on them. Use the datasets at your responsibility and make sure you fulfil the licenses that they were released with. If you use any of the datasets please consider citing the original authors of Place Pulse MIT . Revisit Hyperparameter Optimisation and iterate on model choices using run_name to track design choices.","title":"Dataset"}]}